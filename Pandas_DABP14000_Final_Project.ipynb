{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Pandas! - DABP14000 - 6 September 2022**\n",
        "\n",
        "Group Members: \n",
        "\n",
        "Przemyslaw Zajac,\n",
        "Ming Xie,\n",
        "Erin Criste \n"
      ],
      "metadata": {
        "id": "WTH_q7Ody2nY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary** \n",
        "\n",
        "## Project Goal:\n",
        "\n",
        "This project is designed to identify the possible factors that affect the full-time data scientists’ salaries in US and predict the salaries based on the employee’s features (experience level, job title, residence, etc.) and the employer’s features (company size, remote ratio, etc.). The project goal is to estimate how much salary a data scientist employee can expect to get as accurately as possible using statistical models based on their professional situation. \n",
        "\n",
        "## Project target audience:\n",
        "\n",
        "This project is meant to benefit the people who are interested in starting their career in the data science field like new grads with data-related majors, and who have been working as a data scientist and looking for career development. They can evaluate their current offer, predict their future income and improve their career plans by using this project. \n",
        "\n",
        "The project can also benefit a variety of employers that plan to hire data scientist employees to estimate the hiring costs. \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SBroGmJZTByJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kjCJutrXNoRG"
      },
      "outputs": [],
      "source": [
        "# Allows multiple outputs from a single cell:\n",
        "from IPython.core.interactiveshell import InteractiveShell as IS; IS.ast_node_interactivity = \"all\"\n",
        "!pip -q install -U statsmodels > log.txt   # ensures no FutureWarnings from statsmodels\n",
        "\n",
        "import numpy as np, pandas as pd, matplotlib.pyplot as plt, scipy, seaborn as sns, statsmodels.api as sm, pprint\n",
        "import sklearn, platform, matplotlib, datetime\n",
        "from sklearn.model_selection import train_test_split as tts\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, auc, confusion_matrix\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "#Point to github repo for datasource\n",
        "url = 'https://raw.githubusercontent.com/erinlc27/DABP14000_Fall2022/main/ds_salaries.csv'\n",
        "\n",
        "df = pd.read_csv(url)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Print list of columns\n",
        "print(list(df.columns))"
      ],
      "metadata": {
        "id": "Pl5RK7eTQY1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Drop duplicate index imported\n",
        "df.drop(columns=['Unnamed: 0'],inplace=True)"
      ],
      "metadata": {
        "id": "K21cS7HD9lZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Display 1st 10 observations\n",
        "display(df.iloc[:10])"
      ],
      "metadata": {
        "id": "I8LzlgDoR4TH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Look at column types\n",
        "print(display(df.info()))"
      ],
      "metadata": {
        "id": "dgfvWIJMSOYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Exploratory Data Analysis (EDA)**\n",
        "\n",
        "#1. Dataset Description \n",
        "\n",
        "###*Data Scientist Job Salaries: Click here to view/download [dataset](https://www.kaggle.com/datasets/ruchi798/data-science-job-salaries)*\n",
        "\n",
        "## The Data Scientist Job Salaries dataset was downloaded from Kaggle and contains 607 observations and 12 columns. Each observation provides the employment details of data scientists including experience, salary, and job title to name a few. See below for the column names and description.\n",
        "\n",
        "```\n",
        "Column                   Description\n",
        "-------------------------------------------------------------------------------\n",
        "work_year                The year the salary was paid.\n",
        "experience_level         The experience level in the job during the year with the following possible values: EN Entry-level, Junior MI Mid-level, Intermediate SE Senior-level, Expert EX Executive-level, and Director\n",
        "employment_type          The type of employment for the role: PT Part-time FT, FT Full-time, CT Contract, and FL Freelance\n",
        "job_title                The role worked in during the year.\n",
        "salary                   The total gross salary amount paid.\n",
        "salary_currency          The currency of the salary paid as an ISO 4217 currency code.\n",
        "salaryinusd              The salary in USD (FX rate divided by avg. USD rate for the respective year via fxdata.foorilla.com).\n",
        "employee_residence       Employee's primary country of residence in during the work year as an ISO 3166 country code.\n",
        "remote_ratio             The overall amount of work done remotely, possible values are as follows: 0 No remote work (less than 20%), 50 Partially remote, and 100 Fully remote (more than 80%)\n",
        "company_location         The country of the employer's main office or contracting branch as an ISO 3166 country code.\n",
        "company_siz              The average number of people that worked for the company during the year: S less than 50 employees (small), M 50 to 250 employees (medium), and L more than 250 employees (large)\n",
        "```\n",
        "\n",
        "The dependent variable used for this analysis is the **SALARY in USD**."
      ],
      "metadata": {
        "id": "9LxJ8wz6cthE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Filter dataset to Full Time Employment & company location of US\n",
        "#Full time employment covers 588 out of 607 observations in the dataset thus we felt it represents most of our observations and filtered out Non-FT observations\n",
        "#In the initial model we are are only going to explore US companies Data Scientist salary data\n",
        "df_flt = df.loc[ (df['employment_type'] == 'FT') & (df['company_location'] == 'US') ]\n",
        "df_flt = df_flt.drop(['salary'], axis=1) #Drop salary column as our dependant variable will be salary in USD"
      ],
      "metadata": {
        "id": "AlzmwfAUL0gn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Look at high level data stats\n",
        "dfStat = df_flt.describe()\n",
        "#Drop count stat\n",
        "dfStat.drop(index=['count'],inplace=True)\n",
        "#Drop work_year column, stats for this column are not that informative\n",
        "dfStat.drop(columns=['work_year'],inplace=True)\n",
        "\n",
        "dfStat.loc['IQR'] = dfStat.loc['75%'] - dfStat.loc['25%']\n",
        "dfStat = dfStat.append(df.reindex(dfStat.columns, axis=1).agg(['skew', 'mad', 'kurt']))\n",
        "\n",
        "mapper = {'salary':'{0:.2f}', 'salary_in_usd':'{0:.2f}', 'remote_ratio':'{0:.0f}'}\n",
        "print(display(dfStat.style.format(mapper)))"
      ],
      "metadata": {
        "id": "AH48dV2rSgdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Dataset Features\n",
        "###To analyze our dataset for our problem statement, we chose to filter to full time employment observations only. Full time employment covers 588 out of 607 observations in the dataset thus we felt it represents most of our observations. We filtered the data to include ONLY United States (US) observations which resulted in 355 records from the overall 607. To reduce the complexity of the initial models we chose to only concentrate on US based employers. We also removed salary feature as our dependant variable will be salary in USD.\n",
        "\n",
        "###In addition, there were no NULL or missing values in this dataset.\n",
        "\n",
        "#3. Data Types\n",
        "###The dataset contains several categorical variables that increase the complexity of the analysis including Experience Level (4 types), Remote Ratio (3 types), Company Size (3 types), and Job Title (40 types). For initial modeling purposes, we will exclude the job titles for easier analysis. The experience level and company size will likely need to be categorized into numeric values for the analysis. \n",
        "\n",
        "###From initial exploration of the dataset, we believe that the variables including experience level, salaries in USD, remote ratio, and company size will be the ones to drive our analysis.\n"
      ],
      "metadata": {
        "id": "iSqaR4LYqU9G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#look at KDE Plot\n",
        "pair_plot = sns.pairplot(df_flt, diag_kind='kde',plot_kws={\"s\": 3});\n",
        "pair_plot.fig.suptitle(\"Pairwise Distributions\");"
      ],
      "metadata": {
        "id": "XpTxIm9jSrqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Look at correlation matrix for continious variables\n",
        "plt.figure(figsize=(12, 9))\n",
        "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
        "heatmap = sns.heatmap(df_flt.corr(),annot=True, fmt='.2g', cmap=cmap)\n",
        "heatmap.set_title('Correlation Heatmap', fontdict={'fontsize':12}, pad=12);\n"
      ],
      "metadata": {
        "id": "_MJQjUKyJbZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. Correlation\n",
        "Correlation matrix, histograms, scatterplots, and heatmaps were used to further explore the filtered dataset to begin our initial modeling and analysis.\n",
        "\n",
        "###Histograms and Scatterplots\n",
        "The Histograms show skew within the dataset. Work year and remote ratio both are multi-modal plots. This makes sense as these features are categorical variables represented by numerics.\n",
        "\n",
        "Salary_in_USD has right skew with long tail. \n",
        "\n",
        "From the scatterplots, there does not appear to be any linear relationships within the data.\n",
        "\n",
        "###Correlation Matrix Heatmaps\n",
        "From the correlation matrix in above heatmap, we see no correlation with the data (extremely low R-values)."
      ],
      "metadata": {
        "id": "0RC8To4BqZLf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Baseline Model**"
      ],
      "metadata": {
        "id": "26VDmC73Byi6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Build simple linear regression baseline model\n",
        "\n",
        "#Filter dataframe to columns that will be used to build regression model\n",
        "df_slrm = df_flt.drop(columns=['work_year','employment_type','job_title','salary_currency','employee_residence','company_location'])\n",
        "\n",
        "#Define response variable\n",
        "Y = df_slrm['salary_in_usd']\n",
        "\n",
        "#Drop response variable from feature set\n",
        "X = df_slrm.drop(['salary_in_usd'], axis=1)\n",
        "\n",
        "#Encode categorical feature and drop one column do to multi-collinearity\n",
        "dummy_experience = pd.get_dummies(X['experience_level'], prefix='exp', drop_first=True)\n",
        "dummy_compsize = pd.get_dummies(X['company_size'], prefix='size', drop_first=True)\n",
        "\n",
        "#Drop columns that are replaced by dummy variables\n",
        "X.drop(columns=['experience_level','company_size'],inplace=True)\n",
        "\n",
        "#Assign dummy variables back to the dataframe\n",
        "X = X.join(dummy_experience)\n",
        "X = X.join(dummy_compsize)\n",
        "\n",
        "#Split observations\n",
        "tX, vX, tY, vY = tts(X, Y, test_size = 0.2, random_state=0)\n",
        "#Fit the model\n",
        "mdl = sm.OLS(tY, tX).fit()\n",
        "#Print OLS Regression summary with significance level of 5%\n",
        "print(mdl.summary(title='Baseline Model for Full Time Data Scientist Salary in US', alpha=.05))\n",
        "#Predict salaries using test set \n",
        "pY = mdl.predict(vX)\n",
        "#Compute out-of-sample R^2\n",
        "print(f'Out of sample R^2 is {r2_score(vY, pY):.2f}')"
      ],
      "metadata": {
        "id": "cJ_csW2fhZzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   This is pretty poor out-of-sample R2, let's try to add an intercept to the model to see if it improves\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4tKGh8W6k4jf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Add an intercept to the linear regression model to see if we can improve the out-of-sample R2\n",
        "#Copy dataframe\n",
        "X2 = X.copy()\n",
        "\n",
        "#Define response variable\n",
        "Y2 = Y.copy()\n",
        "\n",
        "#Split observations\n",
        "tX2, vX2, tY2, vY2 = tts(X2, Y2, test_size = 0.2, random_state=0)\n",
        "#Fit the model\n",
        "mdl2 = sm.OLS(tY2, sm.add_constant(tX2)).fit()\n",
        "#Print summary stats\n",
        "print(mdl2.summary(title='Improvement Linear Regression Model for Full Time Data Scientist Salary in US', alpha=.05))\n",
        "#Predict market value using test set \n",
        "pY2 = mdl2.predict(sm.add_constant(vX2))\n",
        "#Compute out-of-sample R^2\n",
        "print(f'Out of sample R^2 is {r2_score(vY2, pY2):.2f}')"
      ],
      "metadata": {
        "id": "G0vh2xwKkojL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Slightly better out-of-sample R2 but still relatively poor model. \n",
        "\n",
        "*   We did not use clustering or logistic regression models as we did not feel it fit our problem statement\n",
        "\n",
        "*   Let's try Decision Tree Regression\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "08J436BzlR3H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Lets try decision tree regressor\n",
        "#Copy dataframe\n",
        "X3 = X.copy()\n",
        "\n",
        "#Define response variable\n",
        "Y3 = Y.copy()\n",
        "\n",
        "tX3, vX3, tY3, vY3 = tts(X3, Y3, test_size=0.2, random_state=0)\n",
        "\n",
        "#Define grid search params\n",
        "param_grid = [{\"max_depth\":[3, 4, 5, None], \"max_features\":[4,5,6]}]\n",
        "\n",
        "gs = GridSearchCV(estimator=DecisionTreeRegressor(random_state=0),\\\n",
        "                 param_grid = param_grid,\\\n",
        "                 cv=10);\n",
        "\n",
        "#Fit the model\n",
        "gs.fit(tX3, tY3)\n",
        "\n",
        "#Print out best combination or params\n",
        "print(f'\\nBest model params: {gs.best_params_}')\n",
        "\n",
        "#Print out best estimator \n",
        "print(f'Best model estimator: {gs.best_estimator_}')\n",
        "\n",
        "#Make a prediction \n",
        "predictions = gs.predict(vX3)\n",
        "\n",
        "#Calculate MAE\n",
        "print(f'MAE : {mean_absolute_error(vY3, predictions)}')\n",
        "\n",
        "#Calculate R2\n",
        "print(f'Out of Sample R2: {r2_score(vY3, predictions)}')"
      ],
      "metadata": {
        "id": "7QYWQSFfwyTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision Tree Regression model did not improve out-of-sample R2. Next we will try Random Forest model."
      ],
      "metadata": {
        "id": "VmwTW9uSInzO"
      }
    }
  ]
}